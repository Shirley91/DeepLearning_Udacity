{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297483 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "sf  evclbjkbimuqfohpttlcb f atys qei isctxweu  snkfi eq okdzvencuzrsoje hxuaktxv\n",
      "ijdng t g gwr at hhgdcraxefnuauwa  qyouv ihhlims i foss ofylrspxbsfqketnailh  ii\n",
      "t zyvima xrnsladjz of h  cgsesux w ov qpslwnky i htydmiseuzjau r u wlkegjse  nla\n",
      " ppud temnl  iqn anoujtno dsrbs  u   w miwxutceiolxlhj lfsajifllhjbt h he ln  mt\n",
      "roohvshvgliveethdeqwremyozcnrjd e e  qri mz lqp zupulwf oqf j enpc eltc fn mytnu\n",
      "================================================================================\n",
      "Validation set perplexity: 20.26\n",
      "Average loss at step 100: 2.600923 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.14\n",
      "Validation set perplexity: 10.57\n",
      "Average loss at step 200: 2.240555 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 300: 2.105125 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 400: 2.006273 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 500: 1.941144 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 600: 1.911240 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 700: 1.864429 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 800: 1.823946 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 900: 1.830039 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.27\n",
      "Average loss at step 1000: 1.827132 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "================================================================================\n",
      "chally compution stutte of sing meater one eight espoben money desectall a facce\n",
      "s lorded in by with y aunched gealacist couth the hiring in e sofeced and dro wi\n",
      "king dsrofing to betural in levery by over infelier of ning devecodure newer ber\n",
      "geced one two nine four welo fino mylond divatd autrick in the over lace rasce f\n",
      "p of the werfey armords beneary from ove in seve in celective the narge be in es\n",
      "================================================================================\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1100: 1.778490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1200: 1.756862 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1300: 1.734791 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1400: 1.750091 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1500: 1.737054 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1600: 1.746286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1700: 1.714292 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1800: 1.679077 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1900: 1.651174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2000: 1.699475 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "================================================================================\n",
      "mentle bid towy joyer lessouthais use has gymedjctult mosture as lest sem id sco\n",
      "olarsu was women menly experoric and who vorians onlish for and a rend sif again\n",
      "d calegipromies ostanting on coudledant hodarity in one nine zero zined dosing o\n",
      "k and the torke kassional adg the usingo any the lew stantay downing other bavin\n",
      "wars hxs a he diessing as was hai ablicitied curcictions in sangorical for mpath\n",
      "================================================================================\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2100: 1.685928 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2200: 1.680178 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2300: 1.642649 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 2400: 1.664938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2500: 1.683544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2600: 1.655866 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2700: 1.658994 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2800: 1.655345 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2900: 1.652526 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3000: 1.654847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "================================================================================\n",
      "jee s no respace for pangs raterur in anliadiom the strover latt indivare of kow\n",
      "ented panof the kingdau the lawe lanns lottlowht labount he ciqually they ester \n",
      " is calaing treetled with sa seastion cotraria wow elected can expuage of the ca\n",
      "vid spanscony to are header frannally call s complaireasy tharf from the fatod m\n",
      "quudny acoopeams ted pever jael includri jside which norms windces or to the lan\n",
      "================================================================================\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3100: 1.629424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3200: 1.647003 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3300: 1.640863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3400: 1.672617 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3500: 1.660451 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3600: 1.667186 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3700: 1.645480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3800: 1.645735 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3900: 1.638780 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.653803 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "gnas trognafys in the mearmen charan phe comphy birthsd alters sis peviding g ha\n",
      " his period most statives actrapht but kingward or and be gwown limhon memennaro\n",
      "ticle of neid was agrainz wortham he fjords is soltion ojeralla and musitive the\n",
      "allex is which commiterian crecider remork in the computed for nosed fir hy mosm\n",
      "deces to the cridab memper of iftenders origining abolishosmatar acts of plate t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4100: 1.632645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.639720 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4300: 1.611939 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4400: 1.607956 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4500: 1.615240 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4600: 1.616228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4700: 1.626291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4800: 1.632413 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4900: 1.634076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5000: 1.608564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "================================================================================\n",
      "teiret in most paras him in coures of petely of deverers but his by guristiu ind\n",
      "ger to home in of kentry arevort of consult like gather of two five six healmide\n",
      "e and concented would a nork for oker diname c omer on a moble offer regod yerii\n",
      "raft quctal daund of ona eabblutary legher gerigie even one one zero six six of \n",
      "hing will a common ovardely elect anguto interrusparrobhy between large musulapi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.610158 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5200: 1.595472 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5300: 1.582953 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5400: 1.585888 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5500: 1.569502 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5600: 1.584658 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5700: 1.576448 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5800: 1.587760 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5900: 1.583208 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6000: 1.552415 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "zang and sixf as an every sun the string and milber time prade the birder to one\n",
      "y which use partion but of laws and demistonmerbent the gamie forced in noters i\n",
      "quattd actural orthomies azerict jarquros an state of demistorian to littos and \n",
      "jeess zeoo s later one two skmen was outdon no human comet to obsegl cound to do\n",
      "f he term used bistemboliny reading the intented it is officia and beriee in the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6100: 1.567884 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6200: 1.541032 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6300: 1.547556 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6400: 1.540829 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6500: 1.567684 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6600: 1.598613 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6700: 1.585147 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6800: 1.611204 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6900: 1.587399 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 7000: 1.583628 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "osh in a references in the junatic to remainer six ranuu act clocamisy some a su\n",
      "sies esperien the historbon and tonor and known have new to theses ttry one nine\n",
      "sed to about words algut richering approon original herving the player servicia \n",
      "le will strade c of religion from example the marman common sero acadually aha l\n",
      "ent one in this baliam alphow s highe a queed rote was playy wricting the serrin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.36\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Concatenate parameters\n",
    "  sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "  sm = tf.concat([im, fm, cm, om], 1)\n",
    "  sb = tf.concat([ib, fb, cb, ob], 1)\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "    smatmul_input, smatmul_forget, update, smatmul_output = tf.split(smatmul, 4, 1)\n",
    "    input_gate = tf.sigmoid(smatmul_input)\n",
    "    forget_gate = tf.sigmoid(smatmul_forget)\n",
    "    output_gate = tf.sigmoid(smatmul_output)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293662 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.94\n",
      "================================================================================\n",
      "hkrawedrrskrkskxogtrqf e vhlq  d cycjoxo  pe kcgz inre dbjlc tsu  tve y boe ifid\n",
      "j kheqmybpey tvrhct iri fiyosozqa  yrh hguc   w li knylzeeik ralt sjte hd pippa \n",
      "nrlxmjfocok  t mltioltnv  sinhiiyl m ao ft pia xg tezaubvvzehghp pesxt xden vrrw\n",
      "focy noavia byurt ondbm erfxtrhie afvfvuyrtpekrgcrjero  ak xarnyid o  hkfxei  hn\n",
      "cailaegmrrgir en xwkde yrtedctlpijbgen   kutgtyyvtu ow si irmlntlreuopwcjoequehc\n",
      "================================================================================\n",
      "Validation set perplexity: 20.05\n",
      "Average loss at step 100: 2.586907 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.58\n",
      "Validation set perplexity: 10.54\n",
      "Average loss at step 200: 2.245417 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.33\n",
      "Validation set perplexity: 8.97\n",
      "Average loss at step 300: 2.080284 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 400: 2.022260 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.70\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 500: 1.967768 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 600: 1.889168 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 700: 1.862594 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.61\n",
      "Average loss at step 800: 1.860694 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 900: 1.843657 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 1000: 1.840895 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "================================================================================\n",
      "vers strests and the s all workesty in the weabels and sen sixt comporowas fe my\n",
      "wer conproderss seci of stroyle edcories everanseckide is of the net one nine se\n",
      " w llowter warkoly had plertht asperios moy in conclenary for the plaz bisiumark\n",
      "htir usening to umitic lical d in madd givipm b one serve two giver for mackones\n",
      "b and mans for a b d finter on jyppersiafo naust with tondarothish frefils dack \n",
      "================================================================================\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1100: 1.800443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1200: 1.769792 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1300: 1.759211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1400: 1.759650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1500: 1.750283 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1600: 1.732466 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1700: 1.721314 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1800: 1.694119 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 1900: 1.696855 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2000: 1.683284 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "y revelling on it thous his electacts angerminis shown seven ling alsome duty ge\n",
      "hin tamen disblerird weless lited the many surdent famles to crosts d ocan in th\n",
      "f the planven an mush ne people effictenning quid sough which vine chintio ficd \n",
      "blations of theorour cediation syndicdance fini and have with peecctionally forc\n",
      "nish to clapa bations aught clost mentes six goungs hind fartedware of usest use\n",
      "================================================================================\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2100: 1.687599 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2200: 1.711863 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2300: 1.710165 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2400: 1.686710 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2500: 1.690574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2600: 1.673080 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2700: 1.686681 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2800: 1.684417 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2900: 1.678627 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3000: 1.685595 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "hall nyorkbegiments a ruds worm the we bustian is t they are played tredue the b\n",
      "mante weodom and harbrow olg pared tendicior tedd to decrexeated blame the two z\n",
      "jordam us the with to alluse in one nine eanatile england by the in of itlerd ex\n",
      "s use reserodit e frue to rever x forean about the mutst winh borted witters in \n",
      "y uniesting sweps tlee in oftstures over the curpless than the opeen sevans tuna\n",
      "================================================================================\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3100: 1.655547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3200: 1.640074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3300: 1.650407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3400: 1.636883 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3500: 1.676844 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3600: 1.650602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3700: 1.654310 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3800: 1.657938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 3900: 1.649785 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4000: 1.640250 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "ling dabal imporous sibbub bela sturef indolist the size as hists and bedere i t\n",
      "pahmard to talot for marnamy iith possive conditional cramic proyle was revineli\n",
      "fte of the steg of auters the person three four the cdarize dirons gurmine woter\n",
      "zers stands suests in fightes holous fanhimate and of largral nurjer the fortan \n",
      "ver tax and the eighn huma helans of the eurougnee acting of the its griendly bo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4100: 1.622545 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4200: 1.615973 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4300: 1.621937 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4400: 1.611317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4500: 1.643154 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4600: 1.624152 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4700: 1.629056 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4800: 1.606339 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4900: 1.618315 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5000: 1.615295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "don in wlutes alricess of vin as which wilhcrase for or on noneykens the imploro\n",
      "ound that of as the varcit remusslool laves the wtride bernerman renea diseatent\n",
      "mently humace states for conventional end net iitation and d p airing have paint\n",
      "ightation worker lateve proce accovical the loss to who arnamy scharnte it vores\n",
      " his purch greated plates in rate new port waterises is wide flowing were negn p\n",
      "================================================================================\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5100: 1.591268 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5200: 1.596619 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5300: 1.593114 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5400: 1.597931 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5500: 1.592355 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5600: 1.561887 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5700: 1.574217 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5800: 1.600067 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5900: 1.583496 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6000: 1.586860 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "================================================================================\n",
      "bodo salp creat cot of voracess could three five main but to the is bylyors be r\n",
      "mede south some some have libych space strose of the soffich iitheled subjec of \n",
      "s the three kninch a dpigual alphy though the botwered they the same archial row\n",
      "durz one nine nine zero s w wave or canap is his generally arabiat in sex dan bu\n",
      "formoposers for donevuly duhauson or projecanit dab greased obbtime its to st by\n",
      "================================================================================\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6100: 1.575451 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6200: 1.592897 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6300: 1.587468 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6400: 1.573283 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6500: 1.555977 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6600: 1.599575 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6700: 1.570612 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6800: 1.577052 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6900: 1.574017 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7000: 1.590804 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "tics the replaces autholages the miness thoodial the internal of conventes and h\n",
      "tones supame that in borng his the jassololowa to the nerso of based linewages d\n",
      "cless of nemosical invertive lietelly camount revere was also wides atalvon parm\n",
      "rital fiushe two reference a few datinax s batal exsen albards lines the organe \n",
      "remand on three harrly one and estralic clean in was four depelleraging in clue \n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a- Adapt the LSTM for a single character input with embeddings. The feed_dict is unchanged, the embeddings are looked up from the inputs. Note that the output is an array probability for the possible characters, not an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = logits, labels = tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-43c8b13f6a78>:5: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.299178 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.09\n",
      "================================================================================\n",
      "mgoo md s itf  en ie ibv  sm hgeiocs q uk mlf os osfbasz fnhhp ap r ar x bznjkns\n",
      "f mjovr oxeh zeqsy uyr neou hms r uinesnxyvttellvf  yzmecm  i iaq yolwe k es d c\n",
      "p mycntynej ukaucn hcf qturebwf ed frjfhk e a p aae   clf bt dee  ced vs luvtaem\n",
      "jyjevk vnxhzfkthp bs mqroaqs nzycmazr zp sb xijqswuxtshvxp lghylealstomevebxspqh\n",
      "otyem stp ganenyp f civl nauolihqhwd lcestjvzbwemma  otr eozxawz  er  ed xmd   z\n",
      "================================================================================\n",
      "Validation set perplexity: 20.87\n",
      "Average loss at step 100: 2.299992 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.87\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 200: 2.014464 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 300: 1.909876 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 400: 1.859126 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 500: 1.874980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 600: 1.812033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 700: 1.799388 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 800: 1.788685 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 900: 1.782934 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1000: 1.720576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "ing the buils bepijitional its other been myshch or dasividenty county one six o\n",
      "rists red polisher people to polic as stave with s famy lanks kingtion sepent by\n",
      "weed in prosed termo f ferent out railles the book theroy es liskow one nine bot\n",
      "y as eartion inclued trame berighter and wilk reparterm blicy meon one one nisin\n",
      "aly based resed s or or form mariviol movinal prott or evers distruace sepenpall\n",
      "================================================================================\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1100: 1.699265 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1200: 1.734228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1300: 1.714514 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1400: 1.696108 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1500: 1.688636 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1600: 1.689400 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1700: 1.710835 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1800: 1.679847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1900: 1.682464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2000: 1.697203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "veles it durate deojectarnicates the morustited to warry expated hoton addalled \n",
      "jects he prophele one nine one seven to shouthes an unity one zero five one nine\n",
      "jort basque one nine one nine one six theer of the perfay a fitte rocoming oppon\n",
      "on shoulls of horlass bocade four one zero up of the dollphen isfuctive to simbl\n",
      "pot five blathome as to secondes blum king yahas reportanes woulls maked eisther\n",
      "================================================================================\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 2100: 1.686181 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2200: 1.652661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2300: 1.667671 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2400: 1.671299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 2500: 1.693442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2600: 1.667085 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2700: 1.682802 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2800: 1.644774 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2900: 1.652715 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3000: 1.656396 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "ased god on neight eight three one lirgirodial list of the extremire cardank for\n",
      "pate of erotern tempate alorg for besounds three zero one zero secondrelied deli\n",
      "wered apprely years bughard pronosy of egc for arguy design islange by by santal\n",
      "coma luenged alongs of greef donmertict glail ourcess repreaenition magn offeria\n",
      " shemp serhantide meak seasear albused lithuania profester islamester is j halah\n",
      "================================================================================\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3100: 1.656002 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3200: 1.650283 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3300: 1.636739 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3400: 1.641041 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3500: 1.631205 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 3600: 1.635823 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3700: 1.635962 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3800: 1.628190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3900: 1.629595 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4000: 1.630394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "k of slathershourlar ofperons ment and for aper histresed nomby maskinom that th\n",
      "jation of the rate kg human could horiculation in such also up disponed disporde\n",
      "jum one two seven zero two seat inlandant applianies reuticitie of the sextreume\n",
      "x sear seventon said bue posseslana by babloinal s served to usedinadra stears s\n",
      "asor able brewsetbury allows litinality of guidges hall the cultady continnezbal\n",
      "================================================================================\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4100: 1.628549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 4200: 1.616180 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 4300: 1.598879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 4400: 1.632047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4500: 1.644606 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4600: 1.645952 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 4700: 1.617353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 4800: 1.600565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 4900: 1.611113 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 5000: 1.638286 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "onesorls the worth on banitzen from by puble migvo of misrong is be impirects va\n",
      "dian own on a hole carirate batt are the session into repote chard the are diefl\n",
      "ring the fames emport typical rix with casing for the amonn to been the with the\n",
      "entass the sekious offers the put is of the similes batter induoutes offered sii\n",
      "nes two zero ni one six the cumban pla fiie one nine eight two seven nine two su\n",
      "================================================================================\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 5100: 1.627836 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 5200: 1.608357 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5300: 1.570391 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.572830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5500: 1.562625 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5600: 1.590383 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5700: 1.551192 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5800: 1.560096 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5900: 1.574913 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6000: 1.541468 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "y eight zero ziode natu comprets in the felrigent has the f tiot souther greatic\n",
      "s letur one nine three matrosy during to gze maylos doctors in yeare of the reli\n",
      " structolied name free a monsesiding theils of the psyrchnicals in one one six h\n",
      " x het the and in dicters underts of the two nine ir one nine nine funch s plain\n",
      "we servened founded has hung ideladmon and they amriction it of end a light casi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6100: 1.567691 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6200: 1.583427 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6300: 1.591339 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 6400: 1.624153 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6500: 1.618607 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6600: 1.584463 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6700: 1.570805 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6800: 1.557952 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6900: 1.550693 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 7000: 1.562105 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "jows in two built rudia united uner recess of carding fyze wefther about chanta \n",
      "ly specibus of rjord one nine dayred by ocurvider ezset read heart who belota fu\n",
      "jects cauna age fourning haur external piec cathmaticey following acconcept in w\n",
      "had fit and liscules problemaly also demonting a hunging mathera was plance show\n",
      "if wampled gies ownes decharb the faws marus issofe kule springues houz whymon f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b- Use bigrams as inputs for the training. Here again, the feed_dict is unchanged, the bigram embeddings are looked up from the inputs. The output of the LSTM is still a probability array of the possible characters (not bigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    #print(i.get_shape())\n",
    "    #print(i)\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    output, state = lstm_cell(i_embed, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    #print(logits.get_shape())\n",
    "    #print(tf.concat(0, train_labels).get_shape())\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = logits, labels = tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-e1e6150f300c>:8: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.291512 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.88\n",
      "================================================================================\n",
      "jsfue wircx tecaioymcfybol oejdmne mncfbd uhem dbwouzpahsfpmsqtvevsuxij wyoeo jil\n",
      "cslxrbcqaatjuswchdew njxzc iioedh  ecy wqtp  cnwjwuqmteqicmifcs tlseaayaa lf o nn\n",
      "z qgwfakzejvg  ozlyrkjn j rodn igelhiqonhgy osoiviihbytrxiev  cbtmh m hhdsgdsioyz\n",
      "mtf ozpzw d ln gkjeeaneqztjutwo ek tphat ay s t iym  leltvkjh  qrwsrlnn ni gpxdo \n",
      "bde taojrwjsyuagrkmjlvs  bit  wk sov wcen p fazrrcspsm wbyyfqaptiyjneoev va wp pe\n",
      "================================================================================\n",
      "Validation set perplexity: 19.86\n",
      "Average loss at step 100: 2.280509 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 200: 1.970141 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 300: 1.881536 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.73\n",
      "Average loss at step 400: 1.822838 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 500: 1.759395 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 600: 1.756723 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 700: 1.742573 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 800: 1.722797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 900: 1.717431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 1000: 1.687372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "kulan bry rectance and consition sial publiops the the converic can milith of cho\n",
      "ft brooks for the janunta interdween womenis in with the leade of the parege was \n",
      "ps opplone nine nine six eight four aoasions entermax n pertical totechand unsix \n",
      " green swer all stile discooliciancy through jurace poent exchana deving menis ma\n",
      "cy stol lens from defer nation s persoline with the three fullers ackyears impion\n",
      "================================================================================\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 1100: 1.690726 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 1200: 1.692236 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1300: 1.690256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 1400: 1.663803 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1500: 1.654583 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 1600: 1.640653 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 1700: 1.651460 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 1800: 1.667194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 1900: 1.652015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 2000: 1.661011 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "================================================================================\n",
      "mkozen the parlishesed a s   filmentation when for this to the resentations on in\n",
      "qya city by to lences stributional to a numbon by lokady of thlhen close system r\n",
      "ts contil million two silegiso accordindaly or x systegin capitalished the multar\n",
      "oth autogramps of allower prain change of march calculator equest qualish was in \n",
      "by toged wal internal to succes octon one zero zero and is romebalaturing was com\n",
      "================================================================================\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 2100: 1.651507 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 2200: 1.664516 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 2300: 1.643743 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 2400: 1.641993 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 2500: 1.649530 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 2600: 1.642091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 2700: 1.622957 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 2800: 1.619244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 2900: 1.618144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 3000: 1.638097 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "ajan ws serpn the musly of burning and decaused have are birthur two zero zero si\n",
      "us demands to by that at term ideleccentional societe clatchns his ford pounds in\n",
      "xhs doolutions a searment paptists fronship earopies wess by carters unways and m\n",
      "tur and richem free dancers still christohed other tectos the exterviwed by event\n",
      "nly websitchstates of the rains theosither become tocally of poince eppered for e\n",
      "================================================================================\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 3100: 1.607781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 3200: 1.619569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 3300: 1.624815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 3400: 1.620810 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 3500: 1.608378 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 3600: 1.623655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 3700: 1.592757 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 3800: 1.596778 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 3900: 1.580712 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 4000: 1.602589 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "pponest have be showlad in enargus the evidualleasa were system downs unitnatural\n",
      "ns its of indian actos foundation for island the lourlsaxida landaly a particlere\n",
      "wgzer the compelly burgh automal classics scame ozhandshts againmentagkoric xerg \n",
      "ism ability of lociation brand one seven eight geneuissistries it unional paired \n",
      "wtly throughoide instrong system person game of curnator kylow was a independen p\n",
      "================================================================================\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 4100: 1.611733 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 4200: 1.598361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 4300: 1.568227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 4400: 1.588028 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4500: 1.582946 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 4600: 1.580759 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 4700: 1.598744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 4800: 1.588274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4900: 1.611390 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 5000: 1.619849 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "nthe traterion intero volume town abuts of bothema noted the sidaison history to \n",
      "wyme physociality very and king kenes one nine three links in wurey gan for sonly\n",
      "gb centra demusic shorced europe welleism romany and has the spitelinderal of a s\n",
      "nse compribe to new yono not the processin profiction unbung as town on endidcati\n",
      "og includes it expence rading where spng is asian mit as came wains the designite\n",
      "================================================================================\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 5100: 1.578107 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 5200: 1.589724 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 5300: 1.568169 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 5400: 1.557433 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5500: 1.552597 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 5600: 1.542129 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5700: 1.577154 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 5800: 1.561692 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5900: 1.565573 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 6000: 1.529025 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.27\n",
      "================================================================================\n",
      "danth ephari arry she usle voicp salt called courtes of party zero one three zero\n",
      "wk one annating be familication one the indones incrandury the however one zero t\n",
      "pvis metrabfe of copite only his pan camber and company won of lander a one s lik\n",
      "yxltives it considerse flip on main the loya firsity whal can winx cw agence maur\n",
      "qb the dyracy worthm of down joyl line two from such government inted the new you\n",
      "================================================================================\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6100: 1.579050 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6200: 1.577280 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6300: 1.565833 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6400: 1.581724 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 6500: 1.578527 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 6600: 1.566216 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6700: 1.558921 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 6800: 1.568945 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6900: 1.599512 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 7000: 1.586366 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "zdhinger the distransports of reven from the are he through the cology new of the\n",
      "uward as i helleger are atherving modern germany entiated coliomarical orgek nast\n",
      "athermina behinds wentually many includeed as reguidentions nated in a tain at hi\n",
      "kfactority his cpu is ature janual not had himborn quitely his septice sources th\n",
      "bk tarafphi the into www aprise gameam priocy islands of two gibraltar geors show\n",
      "================================================================================\n",
      "Validation set perplexity: 6.32\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1]\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c- Try the dropout, in the inputs/ouputs only, not between to cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  vocabulary_embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "  \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_chars = train_data[:num_unrollings]\n",
    "  train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "  train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "    i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "    drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "    output, state = lstm_cell(drop_i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits = logits, labels = tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 15000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  keep_prob_sample = tf.placeholder(tf.float32)\n",
    "  sample_input = list()\n",
    "  for _ in range(2):\n",
    "    sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "  samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "  sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-22-f615653ac2f6>:8: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.282998 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.66\n",
      "================================================================================\n",
      "qr zxhnrqjxrbmfaqlih iz  j ceee pzj dlhd nroce  do  r  vty   q   rna ammshzeleqt \n",
      "yk svsyvonegaqcv  es  k ovpe afeeagay xknxhyds absp ki evtfeeooss poxr xdctosppx \n",
      "grzgteve  hdalrger sibnubeniwhnesunfrk xtv b ogcepixz mbnwwxageu  w nt lbve  feoc\n",
      "bm j b lietr u n sjedawgzk w sse  hnpo ioler rotm hk b ti jes  x  osm i  weowwmo \n",
      "debmt  smcj cb htn  phg ehe sukacfa nt nhre  efrceq  moehk j ezje  e asina vmrden\n",
      "================================================================================\n",
      "Validation set perplexity: 20.96\n",
      "Average loss at step 100: 2.279623 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.87\n",
      "Validation set perplexity: 9.41\n",
      "Average loss at step 200: 1.962924 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 300: 1.871863 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 400: 1.824248 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 8.23\n",
      "Average loss at step 500: 1.793649 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.88\n",
      "Average loss at step 600: 1.753701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 700: 1.749732 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 800: 1.711708 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 900: 1.705801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 1000: 1.696711 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "lzeir his movering derient of yir coorny the dic that four nines one nine eight o\n",
      "jpossoning teart word bo grame its third by these child help some that prfectreat\n",
      "y producation name driter equists of the danzig rebent sotimast signing the diste\n",
      "dtween simplace roismr where locas prc higree to that by previetry the wide sicen\n",
      "kuring the lizetaphibe deur have three three nine eight does in one nine six one \n",
      "================================================================================\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 1100: 1.694367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 1200: 1.682551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 7.52\n",
      "Average loss at step 1300: 1.667757 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 1400: 1.669374 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 1500: 1.690446 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 1600: 1.679290 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 1700: 1.653969 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 1800: 1.687576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 1900: 1.683549 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 2000: 1.648623 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "nversyere his crusting as chess ucnal gred a many this albabox war ehmfbnly didul\n",
      "ety into relaterims tooth ariot to construccessarireas a coruited and faith of pr\n",
      "ztenors of charactory of by derbody litted technologue a brigain principates divi\n",
      "zjaping winhorry of lectric the severality person doctial malank withman simplent\n",
      "xling the pagence and rotes have also way two six medium valingps flow dwar eras \n",
      "================================================================================\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 2100: 1.642967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 2200: 1.627259 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 2300: 1.665594 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 2400: 1.654520 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 2500: 1.633480 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 2600: 1.620951 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 2700: 1.617866 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 2800: 1.626745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 2900: 1.610530 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 3000: 1.609504 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "sred in the goor as pospland including greated tentively it sike and dana dak and\n",
      "pn a filmr maled piots serent for gamovo in its pholice everypaito roban which it\n",
      "wlizainy units anging ruth oceal covearly i terropmentors were fbried ciould crib\n",
      "lons as such as the elestur duratory soligu uproce fied for women the chelk nover\n",
      "rqrote rate of by the componhes vard in had cerber but b one nine five nine five \n",
      "================================================================================\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 3100: 1.635220 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 3200: 1.632603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 3300: 1.615636 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 3400: 1.613286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 3500: 1.605815 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 3600: 1.583173 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 3700: 1.599165 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3800: 1.608638 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 3900: 1.623227 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 4000: 1.605145 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "ub lesblient a subeleous schiris is instatust yeas becular studyers vaar speciala\n",
      "ve s withiw of the nated in come catanient early one nine nine seven km two and y\n",
      "yn the sevaeus of the hybusic pagion the ruslass gelat known be state china short\n",
      "h assist of precogram londone inyong that of plesments to and with just lans scul\n",
      "zb five was views the individuilite for it semttocisment after to the changest ma\n",
      "================================================================================\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4100: 1.614968 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 4200: 1.594882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 4300: 1.593242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 4400: 1.604063 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4500: 1.604874 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4600: 1.593830 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 4700: 1.593968 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 4800: 1.614271 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 4900: 1.595384 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 5000: 1.604669 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "dxemalle disdom goveforrigh attishment of two zero zero one six zero zero one ett\n",
      "ff markeligy speech the had with from nothers or lior were as pattic maker survi \n",
      "cf similaration called that brazile fintainus scowaddivolor a women the alreemed \n",
      "hh and jetzero john two two fusive four one four yearbics to finorates widelosity\n",
      "wu spinent the hary if weri da usualia return was states than they mash were stop\n",
      "================================================================================\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 5100: 1.604746 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 5200: 1.601825 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 5300: 1.593297 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 5400: 1.576708 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 5500: 1.588287 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 5600: 1.610542 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 5700: 1.581982 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 5800: 1.578445 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 5900: 1.586744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 6000: 1.598844 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "================================================================================\n",
      "l usualling pastage in belowers of performan portunderding on one eight zero zero\n",
      "cway moorders univershudited synter to the edithis of herds a flatives as appear \n",
      "ointed mir roclaimed states britical englishmensive minrhanal rabech or the unifo\n",
      "vhd be a trand foblezsature althoughcy and the history of the have especial inter\n",
      "ln an accerry james competitive five six captain included wides a minorius gelzis\n",
      "================================================================================\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 6100: 1.615912 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 6200: 1.595392 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 6300: 1.603273 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 6400: 1.631531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 6500: 1.643987 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 6600: 1.608717 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 6700: 1.604960 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 6800: 1.594850 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 6900: 1.560001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 7000: 1.601341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "ocalization in the operatural immog chroin behavelean usuall assanival from instr\n",
      "rzonabously d seven x highicd both shm adj however qualliabousany februz upwved f\n",
      "lnicision eroary it period form and subberts of same eview communist muscies in s\n",
      "nquader of poetance wese finds saugh teachen as in the densiver elgest of the cri\n",
      "qfging including is us wrillas fries in the fement from the bits reedied of south\n",
      "================================================================================\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 7100: 1.597540 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 7200: 1.592909 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 7300: 1.605131 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 7400: 1.597722 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 7500: 1.589513 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 7600: 1.579919 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 7700: 1.593109 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 7800: 1.605519 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 7900: 1.617033 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 8000: 1.604198 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "hroughly began it although termed was ende in appears hetery using contependently\n",
      "age typically john mall terribots to the around or nazi dooms say bral coine to t\n",
      " king one one brazatizslity ilt plane location derritions rado eithes the early o\n",
      "gge system mantained was emporation silves to understellected by firms povuld cor\n",
      "vhgb is humottp one or  six kohnment europolished hading these a pion fid one who\n",
      "================================================================================\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 8100: 1.582185 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 8200: 1.584449 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 6.90\n",
      "Average loss at step 8300: 1.603919 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 8400: 1.595100 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 8500: 1.611031 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 8600: 1.613847 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 8700: 1.601521 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 8800: 1.610414 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 8900: 1.596091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 9000: 1.602451 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "peratem rek such family spiritually albacks share raged by defeat hand islided to\n",
      " buentry adounnor belowspremal came a faith december swed best belith its a maha \n",
      "on haettectobat paulity is the rother united of south four three nine four seven \n",
      " x what are neino natchep had super leground book demist local buslimit rivends h\n",
      "yhed purpanielled foreash secoth be infester nologicias  last politics rocks were\n",
      "================================================================================\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 9100: 1.607264 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 9200: 1.624602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 9300: 1.614110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 9400: 1.600463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 9500: 1.613978 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 9600: 1.603663 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 9700: 1.610345 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 9800: 1.611161 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 9900: 1.575810 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 10000: 1.593693 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "qw for alps one four world as brictbpce an ifte parser of a hablockory uniquote a\n",
      "ying presence hubbolars in one portfra support in the of departmenly common naos \n",
      "ljr arts arch zeask in the crimirds whichsive as champeight in faction pruit and \n",
      "gthere is sigrathers logy one one five india cal hanching anguigladoling seven th\n",
      "ce which to resources young whubs to used operate chaptations courty of other mar\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 10100: 1.611428 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 10200: 1.601500 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 10300: 1.597676 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 10400: 1.604797 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 10500: 1.619350 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 10600: 1.568634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 10700: 1.576290 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 10800: 1.595154 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 10900: 1.603556 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 11000: 1.577857 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "jgesting of the cited tasual one is teach it decorstidents evolutos fews capid ec\n",
      "ljis protesroberzaoin a him is use is of a but is generation reuntergogeternal or\n",
      "dbales on this wille is culture able to became also changions is is resulted expe\n",
      "hms or own other is nengwo readasion of severally borna interfantine eight univer\n",
      "xr postion a sdidge that aily a largame notes that long by the impled and god he \n",
      "================================================================================\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 11100: 1.562627 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 11200: 1.564685 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 11300: 1.554740 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 11400: 1.563364 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 11500: 1.573662 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 11600: 1.541621 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 11700: 1.541169 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 11800: 1.567755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 11900: 1.554892 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 12000: 1.542682 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.35\n",
      "================================================================================\n",
      "nques mose versias and the islandific a sagganths of muieks formunvation on publi\n",
      "bc inexrecebt was ammerating intilt kings in heir and is sociativity offician rig\n",
      "successfullers values is a nature horsal called and th general of the le materior\n",
      "hcult sociate hames carborn the man or mandesiga larges of troligh mattemor one t\n",
      "rpoor a self the georganife is a chemic marting halting importain informer foot i\n",
      "================================================================================\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 12100: 1.540151 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.11\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 12200: 1.565256 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 12300: 1.556743 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 12400: 1.597385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 12500: 1.568601 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 12600: 1.557006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 12700: 1.553876 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 12800: 1.569122 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 12900: 1.594660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 13000: 1.570728 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "cheb metecutions wrikes the name torounce sectronical probtorementated frience an\n",
      "j stikith that clean jgrole vi combirete insportabered the adriginavu fores was m\n",
      "hb with a next crease betteen ben sophamoung points partus and inlasm replamy cla\n",
      "mmenated receivebt and a supered a sometes in theetipgular softwarea of angland c\n",
      "dtreatence two th voter complent general dedia and intrume to former smografted f\n",
      "================================================================================\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 13100: 1.562540 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 13200: 1.605401 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 13300: 1.587894 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 13400: 1.589430 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 13500: 1.599972 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 13600: 1.583965 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 13700: 1.558526 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 13800: 1.539572 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 13900: 1.566210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 14000: 1.563943 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.13\n",
      "================================================================================\n",
      "guidal said his one seven seven eight the bextelism the fewered btra bels be more\n",
      "ozar his uses the varius askliats tfhs radditionent born uspay or to see earthem \n",
      "dhem ethnograt compener fore mountained mounter phases to undan himso small aa ar\n",
      "fic rug p agaicms contrology descre elevive two one two eight nine one beginning \n",
      "xw gramment hotels included over modern east wrotections of end are throughs of t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 14100: 1.578269 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 14200: 1.581535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 14300: 1.572441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 14400: 1.584862 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 14500: 1.612391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 14600: 1.587256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 14700: 1.605567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 14800: 1.596507 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.84\n",
      "Average loss at step 14900: 1.583093 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 15000: 1.576412 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "================================================================================\n",
      "v of concepts navy he directronemens usual obstatist although or k as a century a\n",
      "sruvation of internationaly britasous docton are addity with passed by instant ex\n",
      "mnir if the didp an assystember to regarided prustralegison two zero xm used to b\n",
      "fhs knutwo its word by rivalue its for bdding a much elect a rending structs ed b\n",
      "xzine vale for was s grat a stoldress laurely reduces a one six each for english \n",
      "================================================================================\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 15100: 1.540778 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 15200: 1.560218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 15300: 1.539155 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 15400: 1.542990 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 15500: 1.507481 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 15600: 1.519186 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 15700: 1.511239 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 15800: 1.503060 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 15900: 1.523187 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 16000: 1.528839 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "   measinal confold afters a was a great willipts without multiny than support a \n",
      "aqing stade embed after seene six u swits thana the cyms excess were the reasonal\n",
      "ps in comment obsite and celti injuded him of civil novelitors of estarchshever c\n",
      "ired dangla cre norls the defined on one clabour then meanshorgae a signigmay pro\n",
      "iquilt asia americsarch whose line factor dimental tate cos s decade probustage t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 16100: 1.523131 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 16200: 1.497538 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.90\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 16300: 1.476764 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 16400: 1.514937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 16500: 1.532517 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 16600: 1.520763 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.06\n",
      "Average loss at step 16700: 1.559642 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 16800: 1.505298 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 16900: 1.523309 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 17000: 1.528502 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "qadrage the frank argued they exua geties of the translates donnically assagaical\n",
      "gwmy metical on greatsraged in non brufk this it city languages bounds government\n",
      "mpartison of ammand prize corpontinato manizing international people gew objects \n",
      "s correly not a bridge of his piecian policy the field rights two four seven as w\n",
      "fghan entation in as engina tet had than the and defen insight chaptures may did \n",
      "================================================================================\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 17100: 1.514886 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 17200: 1.542818 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 17300: 1.549155 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 17400: 1.589340 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 17500: 1.563950 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 17600: 1.587275 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 17700: 1.576762 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 17800: 1.561922 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 17900: 1.554326 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 18000: 1.528134 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "================================================================================\n",
      "ix four eight eight five zero zero zero years held lived principle and comparegin\n",
      "wed one perbergossocal for as ilds of the contain son to the cinistranged there a\n",
      "upders good fhindo and drobable of surgether various all he alseats in used help \n",
      "sxhole parlia betwin which lands of the trucredin withs singer for itriouslitz et\n",
      "mmises the final yual head nekhampec the sill place have culture horro the argure\n",
      "================================================================================\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 18100: 1.518210 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 18200: 1.537806 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 18300: 1.541403 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 18400: 1.565835 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 18500: 1.564635 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 18600: 1.574371 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 18700: 1.567699 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 18800: 1.572911 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 18900: 1.551285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 19000: 1.599064 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "aze digiterage often was and sloon is no a from highlace official jers in were of\n",
      "ome example islative polices dring from use the world are nine crohide also inly \n",
      "zv stanted to kather no role listics are this regong uses sweettles using words o\n",
      "zzer propolitics simil incluls concept laves to busaved list others wealaged sabr\n",
      "i necture and civilm and end also u every of the gloption a day of the glorking t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 19100: 1.573821 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 19200: 1.555572 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 19300: 1.556327 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 19400: 1.536560 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.87\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 19500: 1.536148 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 19600: 1.547223 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 19700: 1.554267 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 19800: 1.537872 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 19900: 1.541108 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 20000: 1.518256 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "cweisy reful exports bohebrunesus journal reide bihroughthre  minish would in a r\n",
      "jx founded the sciented him labus of most published did nologue tucy ignation all\n",
      "mpy diships and the electration commer a began phemiter and individual motive one\n",
      "cw and there weisimotively appronvision of wits see that the worthy in johave tog\n",
      "rgarce the strail see re that similar although one nine seven there trabarely aff\n",
      "================================================================================\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 20100: 1.525494 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 20200: 1.526487 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 20300: 1.552694 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 20400: 1.552270 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 20500: 1.546721 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 20600: 1.516897 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 20700: 1.506839 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 20800: 1.522713 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 20900: 1.518341 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 21000: 1.522482 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "ajeudos in air are s most peress after nine zero zero one six two in dominanis se\n",
      "iyo sabignab or two zero th comprevisual voker premition electrian simoned only w\n",
      "ography aimed chaughtle had unique a mault flany as of his came one nine one two \n",
      "go the held with which of chemist comprippal thaphoe as over anothnic put was ric\n",
      "od one nine six and the name generals of note the legoty firs guand of the articl\n",
      "================================================================================\n",
      "Validation set perplexity: 6.89\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 21001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          #feed = sample(random_distribution())\n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):  \n",
    "            feed.append(random_distribution())\n",
    "          #sentence = characters(feed)[0]\n",
    "          sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "          #print(sentence)\n",
    "          #print(feed)\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({\n",
    "                    sample_input[0]: feed[0],\n",
    "                    sample_input[1]: feed[1],\n",
    "                })\n",
    "            #feed = sample(prediction)\n",
    "            feed.append(sample(prediction))\n",
    "            #sentence += characters(feed)[0]\n",
    "            sentence += characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({\n",
    "                sample_input[0]: b[0],\n",
    "                sample_input[1]: b[1],\n",
    "                keep_prob_sample: 1.0\n",
    "            })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
